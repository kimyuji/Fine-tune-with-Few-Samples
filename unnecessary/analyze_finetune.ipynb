{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import PIL\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import backbone\n",
    "from methods.baselinetrain import BaselineTrain\n",
    "from io_utils import parse_args, get_resume_file, get_best_file, get_assigned_file\n",
    "from datasets import miniImageNet_few_shot, ISIC_few_shot, EuroSAT_few_shot, CropDisease_few_shot, Chest_few_shot\n",
    "from data.datamgr import SimpleDataManager, SetDataManager"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, dim, n_way):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(dim, n_way)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def get_pretrained_model(model, method, pretrained_dataset, save_dir):\n",
    "    num_classes = 200\n",
    "    \n",
    "    model_dict = {model: backbone.ResNet10(method=method)}\n",
    "    pretrained_model = BaselineTrain(model_dict[model], num_classes, loss_type='softmax')\n",
    "    \n",
    "    checkpoint_dir = '%s/checkpoints/%s/%s_%s_aug' %(save_dir, pretrained_dataset, model, method)\n",
    "\n",
    "    modelfile = get_resume_file(checkpoint_dir)\n",
    "    state = torch.load(modelfile)['state']\n",
    "\n",
    "    pretrained_model.load_state_dict(state, strict=True)\n",
    "    pretrained_model.cuda()\n",
    "    pretrained_model.train()\n",
    "    \n",
    "    return pretrained_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "image_size = 224\n",
    "iter_num = 1\n",
    "\n",
    "n_way = 5\n",
    "n_support = 5\n",
    "n_query = 15\n",
    "few_shot_params = dict(n_way=n_way, n_support=n_support)\n",
    "\n",
    "dataset_names = [\"CropDisease\"] # [\"miniImageNet_test\", \"CropDisease\", \"EuroSAT\", \"ISIC\", \"ChestX\"]\n",
    "method = 'baseline'\n",
    "model = 'ResNet10'\n",
    "pretrained_dataset = 'miniImageNet'\n",
    "save_dir = './logs'\n",
    "freeze_backbone = False\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print (dataset_name)\n",
    "    if dataset_name == \"miniImageNet\" or dataset_name == \"miniImageNet_test\":\n",
    "        datamgr = miniImageNet_few_shot.SetDataManager(image_size, n_episode=iter_num, n_query=n_query, **few_shot_params)\n",
    "    elif dataset_name == \"CropDisease\":\n",
    "        datamgr = CropDisease_few_shot.SetDataManager(image_size, n_eposide=iter_num, n_query=n_query, **few_shot_params)\n",
    "    elif dataset_name == \"EuroSAT\":\n",
    "        datamgr = EuroSAT_few_shot.SetDataManager(image_size, n_eposide=iter_num, n_query=n_query, **few_shot_params)\n",
    "    elif dataset_name == \"ISIC\":\n",
    "        datamgr = ISIC_few_shot.SetDataManager(image_size, n_eposide=iter_num, n_query=n_query, **few_shot_params)\n",
    "    elif dataset_name == \"ChestX\":\n",
    "        datamgr = Chest_few_shot.SetDataManager(image_size, n_eposide=iter_num, n_query=n_query, **few_shot_params)\n",
    "        \n",
    "    if dataset_name == \"miniImageNet_test\":\n",
    "        novel_loader = datamgr.get_data_loader(aug=False, train=False)\n",
    "    else:\n",
    "        novel_loader = datamgr.get_data_loader(aug=False)\n",
    "    \n",
    "    prt_w = []\n",
    "    prt_wo = []\n",
    "    ft_w = []\n",
    "    ft_wo = []\n",
    "    df = pd.DataFrame(np.zeros([iter_num, 4]), columns=['prt_wo', 'ft_wo', 'prt_w', 'ft_w'])\n",
    "    \n",
    "    for task_num, (x, y) in tqdm(enumerate(novel_loader)):\n",
    "        pretrained_model = get_pretrained_model(model, method, pretrained_dataset, save_dir)\n",
    "        \n",
    "        ####################################################################################\n",
    "        \n",
    "        classifier = Classifier(pretrained_model.feature.final_feat_dim, n_way)\n",
    "        classifier_opt = torch.optim.SGD(classifier.parameters(), lr = 1e-2, momentum=0.9, dampening=0.9, weight_decay=0.001)\n",
    "        classifier.cuda()\n",
    "        classifier.train()\n",
    "        \n",
    "        if freeze_backbone is False:\n",
    "#             var_init = 0.1\n",
    "#             pretrained_model.feature.trunk[1].running_mean.data.fill_(0.)\n",
    "#             pretrained_model.feature.trunk[1].running_var.data.fill_(var_init)\n",
    "\n",
    "#             pretrained_model.feature.trunk[4].BN1.running_mean.data.fill_(0.)\n",
    "#             pretrained_model.feature.trunk[4].BN1.running_var.data.fill_(var_init)\n",
    "#             pretrained_model.feature.trunk[4].BN2.running_mean.data.fill_(0.)\n",
    "#             pretrained_model.feature.trunk[4].BN2.running_var.data.fill_(var_init)\n",
    "\n",
    "#             pretrained_model.feature.trunk[5].BN1.running_mean.data.fill_(0.)\n",
    "#             pretrained_model.feature.trunk[5].BN1.running_var.data.fill_(var_init)\n",
    "#             pretrained_model.feature.trunk[5].BN2.running_mean.data.fill_(0.)\n",
    "#             pretrained_model.feature.trunk[5].BN2.running_var.data.fill_(var_init)\n",
    "#             pretrained_model.feature.trunk[5].BNshortcut.running_mean.data.fill_(0.)\n",
    "#             pretrained_model.feature.trunk[5].BNshortcut.running_var.data.fill_(var_init)\n",
    "\n",
    "#             pretrained_model.feature.trunk[6].BN1.running_mean.data.fill_(0.)\n",
    "#             pretrained_model.feature.trunk[6].BN1.running_var.data.fill_(var_init)\n",
    "#             pretrained_model.feature.trunk[6].BN2.running_mean.data.fill_(0.)\n",
    "#             pretrained_model.feature.trunk[6].BN2.running_var.data.fill_(var_init)\n",
    "#             pretrained_model.feature.trunk[6].BNshortcut.running_mean.data.fill_(0.)\n",
    "#             pretrained_model.feature.trunk[6].BNshortcut.running_var.data.fill_(var_init)\n",
    "\n",
    "#             pretrained_model.feature.trunk[7].BN1.running_mean.data.fill_(0.)\n",
    "#             pretrained_model.feature.trunk[7].BN1.running_var.data.fill_(var_init)\n",
    "#             pretrained_model.feature.trunk[7].BN2.running_mean.data.fill_(0.)\n",
    "#             pretrained_model.feature.trunk[7].BN2.running_var.data.fill_(var_init)\n",
    "#             pretrained_model.feature.trunk[7].BNshortcut.running_mean.data.fill_(0.)\n",
    "#             pretrained_model.feature.trunk[7].BNshortcut.running_var.data.fill_(var_init)\n",
    "\n",
    "#             for name, p in pretrained_model.named_parameters():\n",
    "#                 if 'trunk.7' in name:\n",
    "#                     if 'BN' in name:\n",
    "#                         if 'weight' in name:\n",
    "#                             p.data.fill_(1.)\n",
    "#                         else:\n",
    "#                             p.data.fill_(0.)\n",
    "#                     else:\n",
    "#                         nn.init.kaiming_uniform_(p.data, a=math.sqrt(5)) # p.data[half:,:,:,:]\n",
    "\n",
    "            delta_opt = torch.optim.SGD(filter(lambda p: p.requires_grad, pretrained_model.parameters()), lr = 1e-2, momentum=0.9, dampening=0.9, weight_decay=0.001)\n",
    "        loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "        ####################################################################################\n",
    "        \n",
    "\n",
    "        n_query = x.size(1) - n_support\n",
    "        x = x.cuda()\n",
    "        x_var = Variable(x)\n",
    "\n",
    "        x_a_i = x_var[:,:n_support,:,:,:].contiguous().view( n_way* n_support, *x.size()[2:]) # (25 (5-way * 5-n_support), 3, 224, 224)\n",
    "        x_b_i = x_var[:,n_support:,:,:,:].contiguous().view( n_way* n_query,  *x.size()[2:]) # (75 (5-way * 15-n_qeury), 3, 224, 224)\n",
    "        y_a_i = Variable( torch.from_numpy( np.repeat(range( n_way ), n_support ) )).cuda() # (25,)\n",
    "        y_b_i = Variable( torch.from_numpy( np.repeat(range( n_way ), n_query ) )).cuda() # (75,)\n",
    "        y_b_i = y_b_i.detach().cpu().numpy()\n",
    "\n",
    "        ####################################################################################\n",
    "\n",
    "        support_size = n_way * n_support\n",
    "        finetune_epoch = 100\n",
    "        batch_size = 4\n",
    "        \n",
    "        for epoch in range(finetune_epoch):\n",
    "            \n",
    "            if epoch == 0:\n",
    "                with torch.no_grad():\n",
    "                    pretrained_model.eval()\n",
    "                    classifier.eval()\n",
    "                    \n",
    "                    w_clas_scores = classifier(pretrained_model.feature(x_b_i))\n",
    "                    topk_scores, topk_labels = w_clas_scores.data.topk(1, 1, True, True)\n",
    "                    topk_ind = topk_labels.cpu().numpy()\n",
    "\n",
    "                    top1_correct = np.sum(topk_ind[:,0] == y_b_i)\n",
    "                    correct_this, count_this = float(top1_correct), len(y_b_i)\n",
    "                    prt_w.append(correct_this/count_this*100)\n",
    "                    # print ('prt w clas {:2.4f}'.format(correct_this/count_this*100))\n",
    "                    \n",
    "                    nil_cls = torch.zeros([5, 512])\n",
    "                    for i in range(5):\n",
    "                        cls_idx = y_a_i==i\n",
    "                        nil_cls[i] = torch.mean(pretrained_model.feature(x_a_i)[cls_idx].cpu(), dim=0)\n",
    "                    wo_clas_scores = torch.mm(pretrained_model.feature(x_b_i).cpu(), nil_cls.T)\n",
    "\n",
    "                    topk_scores, topk_labels = wo_clas_scores.data.topk(1, 1, True, True)\n",
    "                    topk_ind = topk_labels.cpu().numpy()\n",
    "\n",
    "                    top1_correct = np.sum(topk_ind[:,0] == y_b_i)\n",
    "                    correct_this, count_this = float(top1_correct), len(y_b_i)\n",
    "                    prt_wo.append(correct_this/count_this*100)\n",
    "                    # print ('prt w/o clas {:2.4f}'.format(correct_this/count_this*100))\n",
    "                        \n",
    "            pretrained_model.train()\n",
    "            classifier.train()\n",
    "\n",
    "            if freeze_backbone:\n",
    "                pretrained_model.eval()\n",
    "\n",
    "            rand_id = np.random.permutation(support_size)\n",
    "            for j in range(0, support_size, batch_size):\n",
    "                classifier_opt.zero_grad()\n",
    "                if freeze_backbone is False:\n",
    "                    delta_opt.zero_grad()\n",
    "                #####################################\n",
    "                selected_id = torch.from_numpy( rand_id[j: min(j+batch_size, support_size)]).cuda()\n",
    "                z_batch = x_a_i[selected_id]\n",
    "                y_batch = y_a_i[selected_id] \n",
    "                #####################################\n",
    "                output = pretrained_model.feature(z_batch)\n",
    "                scores = classifier(output)\n",
    "                loss = loss_fn(scores, y_batch)\n",
    "                #####################################\n",
    "                loss.backward()\n",
    "                classifier_opt.step()\n",
    "                if freeze_backbone is False:\n",
    "                    delta_opt.step()\n",
    "\n",
    "            pretrained_model.eval()\n",
    "            classifier.eval()\n",
    "            scores = classifier(pretrained_model.feature(x_b_i.cuda()))\n",
    "\n",
    "            topk_scores, topk_labels = scores.data.topk(1, 1, True, True)\n",
    "            topk_ind = topk_labels.cpu().numpy()\n",
    "\n",
    "            top1_correct = np.sum(topk_ind[:,0] == y_b_i)\n",
    "            correct_this, count_this = float(top1_correct), len(y_b_i)\n",
    "            # print (correct_this / count_this *100)\n",
    "            \n",
    "            if epoch == finetune_epoch-1:\n",
    "                with torch.no_grad():\n",
    "                    pretrained_model.eval()\n",
    "                    classifier.eval()\n",
    "                    \n",
    "                    w_clas_scores = classifier(pretrained_model.feature(x_b_i))\n",
    "                    topk_scores, topk_labels = w_clas_scores.data.topk(1, 1, True, True)\n",
    "                    topk_ind = topk_labels.cpu().numpy()\n",
    "\n",
    "                    top1_correct = np.sum(topk_ind[:,0] == y_b_i)\n",
    "                    correct_this, count_this = float(top1_correct), len(y_b_i)\n",
    "                    ft_w.append(correct_this/count_this*100)\n",
    "                    # print ('ft w clas {:2.4f}'.format(correct_this/count_this*100))\n",
    "                    \n",
    "                    nil_cls = torch.zeros([5, 512])\n",
    "                    for i in range(5):\n",
    "                        cls_idx = y_a_i==i\n",
    "                        nil_cls[i] = torch.mean(pretrained_model.feature(x_a_i)[cls_idx].cpu(), dim=0)\n",
    "                    wo_clas_scores = torch.mm(pretrained_model.feature(x_b_i).cpu(), nil_cls.T)\n",
    "\n",
    "                    topk_scores, topk_labels = wo_clas_scores.data.topk(1, 1, True, True)\n",
    "                    topk_ind = topk_labels.cpu().numpy()\n",
    "\n",
    "                    top1_correct = np.sum(topk_ind[:,0] == y_b_i)\n",
    "                    correct_this, count_this = float(top1_correct), len(y_b_i)\n",
    "                    ft_wo.append(correct_this/count_this*100)\n",
    "                    # print ('ft w/o clas {:2.4f}'.format(correct_this/count_this*100))\n",
    "    \n",
    "    df['prt_wo'] = prt_wo\n",
    "    df['ft_wo'] = ft_wo\n",
    "    df['prt_w'] = prt_w\n",
    "    df['ft_w'] = ft_w\n",
    "    df.to_csv('./NIL_results/{}_LBreinit.csv'.format(dataset_name), index=False)\n",
    "    \n",
    "    print ('prt_w mean: {:2.4f}, 95% conf.: {:2.4f}'.format(np.mean(prt_w), 1.96*np.std(prt_w)/np.sqrt(iter_num)))\n",
    "    print ('prt_wo mean: {:2.4f}, 95% conf.: {:2.4f}'.format(np.mean(prt_wo), 1.96*np.std(prt_wo)/np.sqrt(iter_num)))\n",
    "    print ('ft_w mean: {:2.4f}, 95% conf.: {:2.4f}'.format(np.mean(ft_w), 1.96*np.std(ft_w)/np.sqrt(iter_num)))\n",
    "    print ('ft_wo mean: {:2.4f}, 95% conf.: {:2.4f}'.format(np.mean(ft_wo), 1.96*np.std(ft_wo)/np.sqrt(iter_num)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "Relative change of pre-trained network layers "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "image_size = 224\n",
    "iter_num = 600\n",
    "\n",
    "n_way = 5\n",
    "n_support = 5\n",
    "n_query = 15\n",
    "few_shot_params = dict(n_way=n_way, n_support=n_support)\n",
    "\n",
    "dataset_names = [\"miniImageNet_test\", \"CropDisease\", \"EuroSAT\", \"ISIC\", \"ChestX\"]\n",
    "method = 'baseline'\n",
    "model = 'ResNet10'\n",
    "pretrained_dataset = 'miniImageNet'\n",
    "save_dir = './logs'\n",
    "freeze_backbone = False\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print (dataset_name)\n",
    "    if dataset_name == \"miniImageNet\" or dataset_name == \"miniImageNet_test\":\n",
    "        datamgr = miniImageNet_few_shot.SetDataManager(image_size, n_episode=iter_num, n_query=n_query, **few_shot_params)\n",
    "    elif dataset_name == \"CropDisease\":\n",
    "        datamgr = CropDisease_few_shot.SetDataManager(image_size, n_eposide=iter_num, n_query=n_query, **few_shot_params)\n",
    "    elif dataset_name == \"EuroSAT\":\n",
    "        datamgr = EuroSAT_few_shot.SetDataManager(image_size, n_eposide=iter_num, n_query=n_query, **few_shot_params)\n",
    "    elif dataset_name == \"ISIC\":\n",
    "        datamgr = ISIC_few_shot.SetDataManager(image_size, n_eposide=iter_num, n_query=n_query, **few_shot_params)\n",
    "    elif dataset_name == \"ChestX\":\n",
    "        datamgr = Chest_few_shot.SetDataManager(image_size, n_eposide=iter_num, n_query=n_query, **few_shot_params)\n",
    "        \n",
    "    if dataset_name == \"miniImageNet_test\":\n",
    "        novel_loader = datamgr.get_data_loader(aug=False, train=False)\n",
    "    else:\n",
    "        novel_loader = datamgr.get_data_loader(aug=False)\n",
    "        \n",
    "    params_list = [\"Stem.Conv.weight\", \"Stem.BN.scale\", \"Stem.BN.shift\",\n",
    "                   \"Block1.Conv1.weight\", \"Block1.BN1.scale\", \"Block1.BN1.shift\", \"Block1.Conv2.weight\", \"Block1.BN2.scale\", \"Block1.BN2.shift\",\n",
    "                   \"Block2.Conv1.weight\", \"Block2.BN1.scale\", \"Block2.BN1.shift\", \"Block2.Conv2.weight\", \"Block2.BN2.scale\", \"Block2.BN2.shift\", \"Block2.ShortCutConv.weight\", \"Block2.ShortCutBN.scale\", \"Block2.ShortCutBN.shift\",\n",
    "                   \"Block3.Conv1.weight\", \"Block3.BN1.scale\", \"Block3.BN1.shift\", \"Block3.Conv2.weight\", \"Block3.BN2.scale\", \"Block3.BN2.shift\", \"Block3.ShortCutConv.weight\", \"Block3.ShortCutBN.scale\", \"Block3.ShortCutBN.shift\",\n",
    "                   \"Block4.Conv1.weight\", \"Block4.BN1.scale\", \"Block4.BN1.shift\", \"Block4.Conv2.weight\", \"Block4.BN2.scale\", \"Block4.BN2.shift\", \"Block4.ShortCutConv.weight\", \"Block4.ShortCutBN.scale\", \"Block4.ShortCutBN.shift\",\n",
    "                   \"Classifier.weight\", \"Classifier.bias\"]\n",
    "    df = pd.DataFrame(np.zeros([iter_num, len(params_list)]), columns=params_list)\n",
    "    \n",
    "    for task_num, (x, y) in tqdm(enumerate(novel_loader)):\n",
    "        relative_changed_norm = []\n",
    "\n",
    "        pretrained_model = get_pretrained_model(model, method, pretrained_dataset, save_dir)\n",
    "        \n",
    "        ####################################################################################\n",
    "        \n",
    "        classifier = Classifier(pretrained_model.feature.final_feat_dim, n_way)\n",
    "        classifier_opt = torch.optim.SGD(classifier.parameters(), lr = 1e-2, momentum=0.9, dampening=0.9, weight_decay=0.001)\n",
    "        classifier.cuda()\n",
    "        classifier.train()\n",
    "        \n",
    "        before_extractor = copy.deepcopy(pretrained_model.feature)\n",
    "        before_classifier = copy.deepcopy(classifier)\n",
    "        \n",
    "        if freeze_backbone is False:\n",
    "            for name, p in pretrained_model.named_parameters():\n",
    "                if 'trunk.7' in name:\n",
    "                    if 'shortcut' in name:\n",
    "                        if 'BN' in name:\n",
    "#                             pass\n",
    "                            if 'weight' in name:\n",
    "                                p.data.fill_(1.)\n",
    "                            else:\n",
    "                                p.data.fill_(0.)\n",
    "                        else:\n",
    "#                             pass\n",
    "                            nn.init.kaiming_uniform_(p.data, a=math.sqrt(5))\n",
    "                    else:\n",
    "                        if 'C1' in name:\n",
    "                            pass\n",
    "#                             nn.init.kaiming_uniform_(p.data, a=math.sqrt(5))\n",
    "                        if 'BN1' in name:\n",
    "                            pass\n",
    "#                             if 'weight' in name:\n",
    "#                                 p.data.fill_(1.)\n",
    "#                             else:\n",
    "#                                 p.data.fill_(0.)\n",
    "                        if 'C2' in name:\n",
    "#                             pass\n",
    "                            nn.init.kaiming_uniform_(p.data, a=math.sqrt(5))\n",
    "                        if 'BN2' in name:\n",
    "#                             pass\n",
    "                            if 'weight' in name:\n",
    "                                p.data.fill_(1.)\n",
    "                            else:\n",
    "                                p.data.fill_(0.)\n",
    "\n",
    "            delta_opt = torch.optim.SGD(filter(lambda p: p.requires_grad, pretrained_model.parameters()), lr = 1e-2, momentum=0.9, dampening=0.9, weight_decay=0.001)\n",
    "        loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "        ####################################################################################\n",
    "        \n",
    "\n",
    "        n_query = x.size(1) - n_support\n",
    "        x = x.cuda()\n",
    "        x_var = Variable(x)\n",
    "\n",
    "        x_a_i = x_var[:,:n_support,:,:,:].contiguous().view( n_way* n_support, *x.size()[2:]) # (25 (5-way * 5-n_support), 3, 224, 224)\n",
    "        x_b_i = x_var[:,n_support:,:,:,:].contiguous().view( n_way* n_query,  *x.size()[2:]) # (75 (5-way * 15-n_qeury), 3, 224, 224)\n",
    "        y_a_i = Variable( torch.from_numpy( np.repeat(range( n_way ), n_support ) )).cuda() # (25,)\n",
    "        y_b_i = Variable( torch.from_numpy( np.repeat(range( n_way ), n_query ) )).cuda() # (75,)\n",
    "        y_b_i = y_b_i.detach().cpu().numpy()\n",
    "\n",
    "        ####################################################################################\n",
    "\n",
    "        support_size = n_way * n_support\n",
    "        finetune_epoch = 100\n",
    "        batch_size = 4\n",
    "        \n",
    "        for epoch in range(finetune_epoch):\n",
    "            pretrained_model.train()\n",
    "            classifier.train()\n",
    "\n",
    "            if freeze_backbone:\n",
    "                pretrained_model.eval()\n",
    "\n",
    "            rand_id = np.random.permutation(support_size)\n",
    "            for j in range(0, support_size, batch_size):\n",
    "                classifier_opt.zero_grad()\n",
    "                if freeze_backbone is False:\n",
    "                    delta_opt.zero_grad()\n",
    "                #####################################\n",
    "                selected_id = torch.from_numpy( rand_id[j: min(j+batch_size, support_size)]).cuda()\n",
    "                z_batch = x_a_i[selected_id]\n",
    "                y_batch = y_a_i[selected_id] \n",
    "                #####################################\n",
    "                output = pretrained_model.feature(z_batch)\n",
    "                scores = classifier(output)\n",
    "                loss = loss_fn(scores, y_batch)\n",
    "                #####################################\n",
    "                loss.backward()\n",
    "                classifier_opt.step()\n",
    "                if freeze_backbone is False:\n",
    "                    delta_opt.step()\n",
    "                    \n",
    "        after_extractor = copy.deepcopy(pretrained_model.feature)\n",
    "        after_classifier = copy.deepcopy(classifier)\n",
    "        \n",
    "        for b, a in zip(before_extractor.parameters(), after_extractor.parameters()):\n",
    "            relative_changed_norm.append((torch.norm(b-a)/torch.norm(b)).item())\n",
    "\n",
    "        for b, a in zip(before_classifier.parameters(), after_classifier.parameters()):\n",
    "            relative_changed_norm.append((torch.norm(b-a)/torch.norm(b)).item())\n",
    "            \n",
    "        df.loc[task_num] = relative_changed_norm\n",
    "    df.to_csv('./{}_relative_norm_reinit.csv'.format(dataset_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "params_list = [\"Stem.Conv.weight\", \"Stem.BN.scale\", \"Stem.BN.shift\",\n",
    "               \"Block1.Conv1.weight\", \"Block1.BN1.scale\", \"Block1.BN1.shift\", \"Block1.Conv2.weight\", \"Block1.BN2.scale\", \"Block1.BN2.shift\",\n",
    "               \"Block2.Conv1.weight\", \"Block2.BN1.scale\", \"Block2.BN1.shift\", \"Block2.Conv2.weight\", \"Block2.BN2.scale\", \"Block2.BN2.shift\", \"Block2.ShortCutConv.weight\", \"Block2.ShortCutBN.scale\", \"Block2.ShortCutBN.shift\",\n",
    "               \"Block3.Conv1.weight\", \"Block3.BN1.scale\", \"Block3.BN1.shift\", \"Block3.Conv2.weight\", \"Block3.BN2.scale\", \"Block3.BN2.shift\", \"Block3.ShortCutConv.weight\", \"Block3.ShortCutBN.scale\", \"Block3.ShortCutBN.shift\",\n",
    "               \"Block4.Conv1.weight\", \"Block4.BN1.scale\", \"Block4.BN1.shift\", \"Block4.Conv2.weight\", \"Block4.BN2.scale\", \"Block4.BN2.shift\", \"Block4.ShortCutConv.weight\", \"Block4.ShortCutBN.scale\", \"Block4.ShortCutBN.shift\",\n",
    "               \"Classifier.weight\", \"Classifier.bias\"]\n",
    "    \n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "df = pd.read_csv('./miniImageNet_test_relative_norm.csv', index_col=0)\n",
    "plt.plot(range(len(params_list)), list(df.mean()), label='miniImageNet_test', marker='o')\n",
    "\n",
    "df = pd.read_csv('./CropDisease_relative_norm.csv', index_col=0)\n",
    "plt.plot(range(len(params_list)), list(df.mean()), label='CropDisease', marker='o')\n",
    "\n",
    "df = pd.read_csv('./EuroSAT_relative_norm.csv', index_col=0)\n",
    "plt.plot(range(len(params_list)), list(df.mean()), label='EuroSAT', marker='o')\n",
    "\n",
    "df = pd.read_csv('./ISIC_relative_norm.csv', index_col=0)\n",
    "plt.plot(range(len(params_list)), list(df.mean()), label='ISIC', marker='o')\n",
    "\n",
    "df = pd.read_csv('./ChestX_relative_norm.csv', index_col=0)\n",
    "plt.plot(range(len(params_list)), list(df.mean()), label='ChestX', marker='o')\n",
    "\n",
    "plt.xticks(range(len(params_list)), params_list, rotation=90)\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "plt.savefig('./src/relative_change.pdf', bbox_inches='tight', format='pdf')\n",
    "plt.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CKA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def gram_linear(x):\n",
    "    \"\"\"Compute Gram (kernel) matrix for a linear kernel.\n",
    "\n",
    "    Args:\n",
    "    x: A num_examples x num_features matrix of features.\n",
    "\n",
    "    Returns:\n",
    "    A num_examples x num_examples Gram matrix of examples.\n",
    "    \"\"\"\n",
    "    return x.dot(x.T)\n",
    "\n",
    "\n",
    "def gram_rbf(x, threshold=1.0):\n",
    "    \"\"\"Compute Gram (kernel) matrix for an RBF kernel.\n",
    "\n",
    "    Args:\n",
    "    x: A num_examples x num_features matrix of features.\n",
    "    threshold: Fraction of median Euclidean distance to use as RBF kernel\n",
    "      bandwidth. (This is the heuristic we use in the paper. There are other\n",
    "      possible ways to set the bandwidth; we didn't try them.)\n",
    "\n",
    "    Returns:\n",
    "    A num_examples x num_examples Gram matrix of examples.\n",
    "    \"\"\"\n",
    "    dot_products = x.dot(x.T)\n",
    "    sq_norms = np.diag(dot_products)\n",
    "    sq_distances = -2 * dot_products + sq_norms[:, None] + sq_norms[None, :]\n",
    "    sq_median_distance = np.median(sq_distances)\n",
    "    return np.exp(-sq_distances / (2 * threshold ** 2 * sq_median_distance))\n",
    "\n",
    "\n",
    "def center_gram(gram, unbiased=False):\n",
    "    \"\"\"Center a symmetric Gram matrix.\n",
    "\n",
    "    This is equvialent to centering the (possibly infinite-dimensional) features\n",
    "    induced by the kernel before computing the Gram matrix.\n",
    "\n",
    "    Args:\n",
    "    gram: A num_examples x num_examples symmetric matrix.\n",
    "    unbiased: Whether to adjust the Gram matrix in order to compute an unbiased\n",
    "      estimate of HSIC. Note that this estimator may be negative.\n",
    "\n",
    "    Returns:\n",
    "    A symmetric matrix with centered columns and rows.\n",
    "    \"\"\"\n",
    "    if not np.allclose(gram, gram.T):\n",
    "        raise ValueError('Input must be a symmetric matrix.')\n",
    "    gram = gram.copy()\n",
    "\n",
    "    if unbiased:\n",
    "        # This formulation of the U-statistic, from Szekely, G. J., & Rizzo, M.\n",
    "        # L. (2014). Partial distance correlation with methods for dissimilarities.\n",
    "        # The Annals of Statistics, 42(6), 2382-2412, seems to be more numerically\n",
    "        # stable than the alternative from Song et al. (2007).\n",
    "        n = gram.shape[0]\n",
    "        np.fill_diagonal(gram, 0)\n",
    "        means = np.sum(gram, 0, dtype=np.float64) / (n - 2)\n",
    "        means -= np.sum(means) / (2 * (n - 1))\n",
    "        gram -= means[:, None]\n",
    "        gram -= means[None, :]\n",
    "        np.fill_diagonal(gram, 0)\n",
    "    else:\n",
    "        means = np.mean(gram, 0, dtype=np.float64)\n",
    "        means -= np.mean(means) / 2\n",
    "        gram -= means[:, None]\n",
    "        gram -= means[None, :]\n",
    "\n",
    "    return gram\n",
    "\n",
    "def cka(gram_x, gram_y, debiased=False):\n",
    "    \"\"\"Compute CKA.\n",
    "\n",
    "    Args:\n",
    "    gram_x: A num_examples x num_examples Gram matrix.\n",
    "    gram_y: A num_examples x num_examples Gram matrix.\n",
    "    debiased: Use unbiased estimator of HSIC. CKA may still be biased.\n",
    "\n",
    "    Returns:\n",
    "    The value of CKA between X and Y.\n",
    "    \"\"\"\n",
    "    gram_x = center_gram(gram_x, unbiased=debiased)\n",
    "    gram_y = center_gram(gram_y, unbiased=debiased)\n",
    "\n",
    "    # Note: To obtain HSIC, this should be divided by (n-1)**2 (biased variant) or\n",
    "    # n*(n-3) (unbiased variant), but this cancels for CKA.\n",
    "    scaled_hsic = gram_x.ravel().dot(gram_y.ravel())\n",
    "\n",
    "    normalization_x = np.linalg.norm(gram_x)\n",
    "    normalization_y = np.linalg.norm(gram_y)\n",
    "    return scaled_hsic / (normalization_x * normalization_y)\n",
    "\n",
    "\n",
    "def _debiased_dot_product_similarity_helper(xty, sum_squared_rows_x, sum_squared_rows_y, squared_norm_x, squared_norm_y, n):\n",
    "    \"\"\"Helper for computing debiased dot product similarity (i.e. linear HSIC).\"\"\"\n",
    "    # This formula can be derived by manipulating the unbiased estimator from\n",
    "    # Song et al. (2007).\n",
    "    return (\n",
    "      xty - n / (n - 2.) * sum_squared_rows_x.dot(sum_squared_rows_y)\n",
    "      + squared_norm_x * squared_norm_y / ((n - 1) * (n - 2)))\n",
    "\n",
    "\n",
    "def feature_space_linear_cka(features_x, features_y, debiased=False):\n",
    "    \"\"\"Compute CKA with a linear kernel, in feature space.\n",
    "\n",
    "    This is typically faster than computing the Gram matrix when there are fewer\n",
    "    features than examples.\n",
    "\n",
    "    Args:\n",
    "    features_x: A num_examples x num_features matrix of features.\n",
    "    features_y: A num_examples x num_features matrix of features.\n",
    "    debiased: Use unbiased estimator of dot product similarity. CKA may still be\n",
    "      biased. Note that this estimator may be negative.\n",
    "\n",
    "    Returns:\n",
    "    The value of CKA between X and Y.\n",
    "    \"\"\"\n",
    "    features_x = features_x - np.mean(features_x, 0, keepdims=True)\n",
    "    features_y = features_y - np.mean(features_y, 0, keepdims=True)\n",
    "\n",
    "    dot_product_similarity = np.linalg.norm(features_x.T.dot(features_y)) ** 2\n",
    "    normalization_x = np.linalg.norm(features_x.T.dot(features_x))\n",
    "    normalization_y = np.linalg.norm(features_y.T.dot(features_y))\n",
    "\n",
    "    if debiased:\n",
    "        n = features_x.shape[0]\n",
    "        # Equivalent to np.sum(features_x ** 2, 1) but avoids an intermediate array.\n",
    "        sum_squared_rows_x = np.einsum('ij,ij->i', features_x, features_x)\n",
    "        sum_squared_rows_y = np.einsum('ij,ij->i', features_y, features_y)\n",
    "        squared_norm_x = np.sum(sum_squared_rows_x)\n",
    "        squared_norm_y = np.sum(sum_squared_rows_y)\n",
    "\n",
    "        dot_product_similarity = _debiased_dot_product_similarity_helper(\n",
    "            dot_product_similarity, sum_squared_rows_x, sum_squared_rows_y,\n",
    "            squared_norm_x, squared_norm_y, n)\n",
    "        normalization_x = np.sqrt(_debiased_dot_product_similarity_helper(\n",
    "            normalization_x ** 2, sum_squared_rows_x, sum_squared_rows_x,\n",
    "            squared_norm_x, squared_norm_x, n))\n",
    "        normalization_y = np.sqrt(_debiased_dot_product_similarity_helper(\n",
    "            normalization_y ** 2, sum_squared_rows_y, sum_squared_rows_y,\n",
    "            squared_norm_y, squared_norm_y, n))\n",
    "\n",
    "    return dot_product_similarity / (normalization_x * normalization_y)\n",
    "\n",
    "def cca(features_x, features_y):\n",
    "    \"\"\"Compute the mean squared CCA correlation (R^2_{CCA}).\n",
    "\n",
    "    Args:\n",
    "    features_x: A num_examples x num_features matrix of features.\n",
    "    features_y: A num_examples x num_features matrix of features.\n",
    "\n",
    "    Returns:\n",
    "    The mean squared CCA correlations between X and Y.\n",
    "    \"\"\"\n",
    "    qx, _ = np.linalg.qr(features_x)  # Or use SVD with full_matrices=False.\n",
    "    qy, _ = np.linalg.qr(features_y)\n",
    "    return np.linalg.norm(qx.T.dot(qy)) ** 2 / min(\n",
    "      features_x.shape[1], features_y.shape[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# cka = feature_space_linear_cka()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}